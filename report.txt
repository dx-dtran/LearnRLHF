\documentclass{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{geometry}
\geometry{a4paper, margin=1in}

\title{Fine-Tuning InstructGPT with PPO in the RLHF Stage: A Beginner’s Guide}
\author{}
\date{}

\begin{document}

\maketitle

\section{Introduction}

InstructGPT (the precursor to ChatGPT) is trained not only on vast text data but also through Reinforcement Learning from Human Feedback (RLHF). In the RLHF stage, the model learns to align its answers with human preferences. A key algorithm used here is Proximal Policy Optimization (PPO) – a type of reinforcement learning method. This guide will break down, in simple terms, how PPO is applied to fine-tune the model using human feedback. We’ll cover the basic RL concepts (like state, action, reward in this context), what PPO does and why it’s used, how the PPO loss is applied (per token vs per sequence), and demystify terms like the clipped objective, KL-divergence penalty, and advantage estimation. We’ll also explain how human preference data is turned into a reward signal and how that signal updates the model. Throughout, we’ll use analogies and simple examples to make these concepts clear.

\section{RLHF Basics for Language Models: State, Action, and Reward}

First, let’s map the usual reinforcement learning terms to the language model (InstructGPT) scenario:
\begin{itemize}
    \item \textbf{State}: In RL, the state is the situation the agent finds itself in. For a language model, the state is essentially the text context or prompt the model is given (or the conversation so far). For example, if the user asks a question, that question is the current state from the model’s perspective. As the model generates a response word by word, you could also think of the state at each step as the text generated so far.
    \item \textbf{Action}: An action is what the agent (the model) does in a state. For a language model, the action is generating the next chunk of text. Typically, this means choosing the next token (word or sub-word) in the sequence. For simplicity, you can also view the model’s entire answer to a prompt as the outcome of a sequence of actions (each action being a word added to the answer).
    \item \textbf{Reward}: The reward is a feedback signal telling the agent how well it did. In the RLHF stage of InstructGPT, the reward comes from human feedback. Humans have provided their preference judgments on various model outputs, and a reward model is trained on those preferences. This reward model takes a prompt and an output and returns a single number (a score) that reflects how preferable or good that output is according to human evaluators. A high reward means the model’s answer was closer to what humans prefer (helpful, correct, harmless, etc.), while a low reward means the answer was poor or undesirable. For example, if the model’s task is to be helpful and polite, an answer that is accurate and polite would get a higher reward score than a rude or incoherent answer.
\end{itemize}

How these concepts tie together: Think of the model as an “agent” playing a game where the goal is to produce the best possible answer to any user prompt. The environment in this case is the world of language and prompts the model interacts with. When a prompt (state) is presented, the model (agent) must output text (actions, one token at a time) to form a complete answer. Once the answer is complete (end of the sequence), the environment (here implemented via the reward model) provides a reward signal that judges the answer’s quality. The model then uses this signal to adjust itself. Over many iterations, it learns to generate answers that earn higher rewards, meaning they satisfy human preferences more often.

Analogy: Imagine the model is a student answering questions on an assignment. The state is the question being asked. The student writes an answer (action). A teacher then grades it with a score – that score is the reward. The student doesn’t have the correct answers in advance (no explicit ground truth in many cases), so instead they learn from the teacher’s feedback which kinds of answers score well. Over time, the student adjusts how they answer questions to get better scores. In RLHF, the “teacher” giving the score is the reward model trained from human preferences.

\section{What is PPO and Why Use It in RLHF?}

Proximal Policy Optimization (PPO) is a specific algorithm in the family of policy gradient methods in reinforcement learning. At a high level, PPO is used to fine-tune the model (policy) such that it maximizes the expected reward (in this case, the human-derived reward). In the RLHF stage for InstructGPT, PPO takes the place of the usual training objective – instead of predicting the next word from text data (as in pre-training), the model now tries to produce outputs that get a high score from the reward model.

Why PPO? Because vanilla reinforcement learning methods can be unstable or inefficient for large models. PPO was introduced as a technique to stabilize training by making only small, “proximal” updates to the policy. In practice, PPO has several benefits that make it well-suited for RLHF with language models:
\begin{itemize}
    \item \textbf{Stability through Limited Updates}: PPO avoids making huge changes to the model’s behavior in a single step. It uses a trust region approach – essentially constraining how far the new policy can deviate from the old policy. This is achieved by a clipped objective or a KL-divergence penalty (more on these later). The effect is that the model is updated cautiously, reducing the chance of diverging into nonsensical outputs. This yields low-variance, stable updates. In other words, PPO keeps the model from thrashing about as it learns from feedback.
    \item \textbf{Better Exploration}: PPO often includes an entropy bonus which encourages the model to keep some randomness in its predictions. This helps the model explore different ways of answering rather than getting stuck always giving the same answer. This is important in RLHF because we want the model to discover improved responses, not just stick to its initial habits.
    \item \textbf{Sample Efficiency}: PPO is designed to reuse training data effectively by performing multiple training passes on the same batch of experiences (in our case, prompt/answer/reward tuples). This is useful because collecting data (prompts and human feedback) is costly – we want to get the most learning out of each sample. PPO can learn faster with fewer samples compared to very simple policy gradient methods.
    \item \textbf{Ease of Implementation}: Compared to some other sophisticated RL algorithms (like TRPO, Trust Region Policy Optimization), PPO is simpler to implement while still maintaining good performance. This matters when scaling up to large language models, where implementation complexity can slow down progress.
\end{itemize}

In summary, PPO strikes a balance between making meaningful improvements to the policy and keeping training stable. In the context of aligning a large language model with human preferences, this balance is crucial. We don’t want the model to exploit the reward signal in bizarre ways (like outputting gibberish that the reward model mistakenly scores high), nor do we want training to collapse. PPO (especially with the modifications used in InstructGPT) helps navigate this by clipping overly large updates and penalizing deviations that don’t align with the original language distribution.

Analogy: If the language model is like that student learning from a teacher’s feedback, PPO is like a rule that the student should only make gradual changes to their approach each time they get feedback. Instead of completely rewriting their style after one piece of feedback, they fine-tune it little by little. This prevents wild swings in performance – the student won’t go from writing an essay to drawing a comic just because one comic got a good score. PPO enforces that “don’t change too much at once” rule, ensuring steady improvement.

\section{PPO in Action: Fine-Tuning InstructGPT with Human Feedback}

Let’s walk through how the PPO training loop works in the RLHF phase for InstructGPT:
\begin{enumerate}
    \item \textbf{Initialize from the Supervised Model}: Before RLHF, the model is usually first trained on supervised instruction-following data (this is often called the SFT model, for Supervised Fine-Tuning). This gives us a reasonable starting policy for answering prompts. PPO training starts from this model’s weights (so the policy is already pretty good at basic instruction following). They also create a reference model – basically a frozen copy of the SFT model – which will be used to measure how much the policy changes (for the KL penalty).
    \item \textbf{Encounter a Prompt (State)}: The environment presents a prompt to the model. In practice this means we sample a prompt from a dataset (for example, a question or instruction that a user might ask). In RL terms, this prompt is the state. In the InstructGPT paper they modeled this as a bandit setup – each prompt is a separate episode with no carry-over memory, so you can think of it as a one-step game for each prompt.
    \item \textbf{Model Generates an Answer (Action)}: The model (policy) produces an answer to the prompt. It does this by generating a sequence of tokens one by one. This sequence is the model’s action in response to the state. (If the prompt is “Explain the moon phases,” the model might output a multi-sentence explanation – that whole answer is the action, consisting of many token-level decisions.)
    \item \textbf{Receive a Reward from the Reward Model}: Once the model has given an answer, the reward model (RM) steps in. The reward model looks at the prompt and the generated answer, and outputs a score – a scalar reward. This reward is supposed to reflect how well the answer met human preferences (as learned from human feedback comparisons). For example, if the answer was clear, correct, and helpful, the reward might be high (say +8); if it was irrelevant or offensive, the reward might be low or even negative (e.g. –2). This scalar reward is the signal used to tune the model. After this, the “episode” ends (since in this bandit setup we only get one reward at the end for the whole answer).
    \item \textbf{Calculate Advantage (Was the outcome better or worse than expected?)}: Not all rewards are equally surprising or informative. Here PPO uses a concept called the advantage. We have the model’s answer reward, but we also need to know: did the model expect such a reward? For this, PPO typically uses a value function (a separate model or a head on the policy model) to predict the expected reward given the prompt (state). In InstructGPT, they initialized this value function using the reward model’s outputs, since the reward model already provides a guess of how good an answer is. The advantage is basically: Advantage = (Actual Reward) – (Expected Reward). If the model got more reward than expected (advantage > 0), it means the answer was better than what the current policy thought it would achieve – so the actions taken were good and should be encouraged. If the advantage is negative, the answer was worse than expected – those actions should be discouraged. Computing the advantage helps focus the learning: the model will pay more attention to outcomes that are unexpectedly good or bad, rather than outcomes it already anticipated. (This concept also reduces the variance of training – it’s easier to learn from the difference in outcome rather than the raw outcome, because the difference highlights the unexpected part.)
    \item \textbf{Policy Update with PPO Objective}: Now comes the core step – updating the model’s parameters to increase the chances of good actions and decrease the chances of bad ones. PPO does this using a special objective function. There are a few moving parts here:
    \begin{itemize}
        \item \textbf{Probability Ratio}: PPO looks at the probability the model assigned to its chosen actions before the update versus after. For each token action, we have an old probability (from the current policy before updating) and a new probability (the policy with updated parameters). We form a ratio $r = \text{new\_prob} / \text{old\_prob}$. If $r > 1$, it means the model is now assigning higher probability to that action than before; if $r < 1$, it’s assigning lower probability.
        \item \textbf{Clipped Objective}: Rather than simply maximize the reward times the probability (like a naive policy gradient), PPO uses a clipped objective. In plain terms, it says: if the policy tries to change probability too much (beyond a certain $\pm\varepsilon$ range, e.g. $\pm0.2$ or 20\%), we clip the ratio. Mathematically, the objective includes $\min( r * \text{Advantage},\; \text{clip}(r,\; 1-\varepsilon,\; 1+\varepsilon) * \text{Advantage} )$. This means if $r$ tries to go beyond, say, 1.2 or below 0.8, it will be capped at those values for the purpose of computing the loss. The effect is that if an action was very advantageous (good) the algorithm will increase its probability, but not by more than, say, 20\% in one go. Similarly, if it was very bad, it will decrease the probability, but not drop it to near zero in one step. This clipping keeps updates “proximal” or small, avoiding wild jumps in the policy. It’s a safety check to maintain stability.
        \item \textbf{KL Divergence Penalty}: In the specific RLHF training of InstructGPT, OpenAI added an explicit KL divergence penalty in the reward signal. KL divergence is a measure of how one probability distribution differs from another. Here, they measure how the new policy’s output distribution diverges from the original SFT model’s distribution (the pre-trained supervised policy). If the policy starts to stray too far from the way the SFT model would normally talk, a penalty term subtracts from the reward. Effectively, the adjusted reward = $\text{reward\_model\_score} – \beta * \text{KL}(\text{new\_policy} || \text{reference\_policy})$. Intuitively, this means the algorithm gets less reward if it deviates too much from the style and behavior of the earlier model. This is important to prevent exploitation or “reward hacking.” Without the KL term or clipping, the policy might find strange loopholes to get a high reward model score (for instance, producing very long, repetitive answers if the reward model slightly favors longer responses). The KL penalty reins this in, keeping the new model’s behavior in line with human-like text. It’s a bit like saying: “Improve your answers, but don’t lose the fluent style and correctness you had before.” In fact, the InstructGPT paper notes that without such a penalty, the policy could trick the reward model into giving high scores for nonsense output.
        \item \textbf{Value Function Loss}: PPO is often run in an actor-critic style, meaning it also updates the value function to better predict rewards. The value function gets a loss to minimize the error between its predictions and the actual observed rewards (this ensures our advantage estimates get more accurate over time). We won’t go deep into this, but it’s part of the PPO training loop as well.
        \item \textbf{Entropy Bonus}: As mentioned, sometimes an entropy term is added to encourage the policy to keep exploring. This means we add a small bonus for higher uncertainty (i.e. not putting all probability mass on one token), to avoid the model becoming overly confident too soon. This helps maintain some creativity and exploration.
    \end{itemize}
    Bringing it together, the PPO update adjusts the model’s weights so that actions with positive advantage (better than expected) become more likely, and actions with negative advantage become less likely – but it does so in a controlled way (clipped updates, plus a penalty if it diverges from the reference policy too much). All of these pieces (clipping, KL penalty, advantage) work together to make learning stable. The OpenAI team specifically used a KL-penalty variant of PPO (sometimes called PPO-KL) for InstructGPT, where they tune the coefficient $\beta$ for the KL term.
    \item \textbf{Repeat with New Prompts}: PPO is an on-policy algorithm, so after some updates, the model’s policy changes. We then collect new samples (the model answering new prompts, getting rewards) and continue the training. Steps 2–6 are iterated for many prompts and epochs. Over time, the policy (the InstructGPT model) becomes better at getting high reward model scores, which means it’s better aligned with the human preferences encoded in that reward model.
\end{enumerate}

\textbf{Token-Level vs Sequence-Level Updates}: A common question is whether the PPO loss applies per token or per entire sequence. The reward is obtained for the entire sequence (the whole answer) at the end of the episode – there isn’t an explicit human reward for each individual word, just one score for the complete answer. However, the model made a sequence of token-level actions to generate that answer. In the PPO update, we attribute the reward to all the actions (tokens) in that sequence. In practice, the gradient of the PPO loss is computed by summing over each token’s log-probability, weighted by the advantage. This effectively propagates the final reward back to each token choice. So you can say the PPO objective is applied at each token decision, but it’s using a sequence-level reward signal. Another way to view it: the whole sequence is the “action” that gets a reward, but mathematically that breaks down into a product of token probabilities, so each token’s probability is nudged up or down. Additionally, the KL penalty is applied per token – at each generation step, the policy’s divergence from the reference policy is measured and penalized. This means the model is being gently pulled back towards the supervised policy at every step if it goes too far. In summary, the PPO loss is computed across the sequence but on a per-token basis: the reward is for the complete output, yet the model updates its behavior for each token it chose, since each token contributed to that outcome.

Analogy: Imagine writing an essay (sequence of words) and receiving a single grade (reward) for the whole essay. You don’t get a per-word grade, but when improving, you’ll adjust word choices throughout the essay to raise the overall score. Similarly, the model adjusts the probabilities of each word it wrote, in light of the final score. If a particular word or phrasing likely contributed to a better score, the model will use it more; if it contributed to a bad score, the model will avoid it. The final grade applies to the essay as a whole, but the revisions happen word by word.

\section{Key Concepts in PPO: Clipping, KL Divergence, and Advantage (Intuition and Simple Math)}

Let’s delve a bit more into the core concepts of PPO and demystify them with minimal math and intuitive explanations:
\begin{itemize}
    \item \textbf{Advantage Function} ($A$): This is central to learning signal in PPO. Formally, an advantage $A(s,a)$ is the difference between the actual reward outcome and the expected reward for taking action $a$ in state $s$. In formula, a simple version is:
    \[ A = R - V(s) \]
    where $R$ is the reward obtained and $V(s)$ is the value function’s estimate of reward for state $s$. If $A$ is positive, the action did better than expected; if negative, it did worse. The advantage tells the model whether an action was advantageous or not. Intuition: it’s like a surprise meter – positive if “wow, that went better than I thought,” negative if “that was disappointing.” PPO uses this to scale updates: actions with positive advantage get their probabilities increased, and vice versa. Using an advantage (instead of raw reward) is important because it normalizes feedback against expectation, which makes learning more stable. For example, if the model always expects a reward of 5 but gets a 7, the +2 advantage prompts it to tweak behavior in that direction; if it got a 5 as expected, advantage $\sim 0$, so no big change is needed. In practice, PPO often uses Generalized Advantage Estimation (GAE) to compute a smoothed advantage over multi-step scenarios, but in the bandit setting of InstructGPT (one reward at end), it might boil down to just reward minus baseline. The key point is: advantage isolates the useful learning signal. It answers: was this outcome better or worse than the policy’s current predictions?
    \item \textbf{Clipped Probability Ratio} (PPO’s surrogate objective): In standard policy gradient, one might maximize $ \log \pi_\theta(a|s) * A$ (the log probability of the action times the advantage). PPO instead looks at the ratio $r = \frac{\pi_{\text{new}}(a|s)}{\pi_{\text{old}}(a|s)}$ which indicates how much the new policy is trying to change the probability of action $a$. The clipped objective is usually written as:
    \[ L_{\text{clip}} = -\mathbb{E}\Big[ \min\big( r * A,\; \text{clip}(r,\;1-\varepsilon,\;1+\varepsilon) * A \big) \Big] \]
    Don’t be intimidated by this formula – conceptually it means: we want to maximize the advantage-weighted probability of actions, but if the new probability deviates too much (more than $(1+\varepsilon)$ or less than $(1-\varepsilon)$ of the old probability), we stop it there. The $\min$ inside ensures that if increasing $r$ beyond the clip threshold would give a higher objective, we instead take the clipped value. This effectively creates a flat region in the loss beyond a certain change, so there’s no incentive to go further. Intuition: It’s like telling the model: “Sure, improve this action if it was good, but don’t overdo it.” Likewise, “downgrade this action if it was bad, but don’t eliminate it in one go.” The result is a more conservative update. This clipping is crucial for preventing the training from collapsing. Especially with neural networks, a single update that drastically changes some probabilities can lead to weird consequences. PPO’s clipped objective is a simple way to enforce a trust region (a limit on how far the policy can move in one update) without complicated math. Think of it as training wheels on a bike – it prevents the model from over-correcting and falling over during training.
    \item \textbf{KL Divergence Term} (Policy Regularization): KL divergence measures how different two probability distributions are. In RLHF for InstructGPT, they monitor and penalize the KL divergence between the new policy (the model after updates) and the reference policy (the original SFT model). The training objective actually incorporates this as a subtraction from the reward: effective reward = $\text{human\_reward} – \beta \cdot \text{KL}(\text{new}|| \text{reference})$. The higher the $\beta$ coefficient, the more the model is discouraged from straying from the reference. Intuitively, this is a form of regularization: it keeps the fine-tuning grounded. Because the language model was initially trained on a vast amount of general text (and then supervised fine-tuned), it has a strong prior for fluent, sensible language. We don’t want RLHF to accidentally unlearn that by optimizing too narrowly for the reward model. The KL term acts like a leash: the model can improve to get higher reward, but if it tries to run too far from its prior (its original style and knowledge), the leash (penalty) pulls it back. This also addresses “reward hacking” – cases where the model might exploit quirks of the reward model that wouldn’t actually align with true human intent. For example, if the reward model isn’t perfect, the policy might find a loophole (like always appending a certain phrase that the reward model associates with good answers) – the KL penalty would discourage strategies that diverge strongly from normal language use. In summary, the KL term ensures the model stays honest in its improvements, making the training more robust.
    \item \textbf{Putting it together}: The overall PPO loss combines these elements. In InstructGPT (without the pretraining mix), the objective can be summarized as maximizing:
    \[ \text{Reward Model Score} - \beta \cdot \text{KL}(\text{new}||\text{old}) \]
    (with the clipping applied in how we do the update, and including a value loss and entropy bonus as additional terms).
    The exact formulas can get a bit involved, but the high-level idea is the same: push the policy toward higher reward while keeping it sufficiently close to the original distribution. By understanding advantage, clipping, and KL penalty, you have the core intuition for how PPO is accomplishing the fine-tuning in a safe, stable way.
\end{itemize}

\section{From Human Preferences to Reward Signals}

You might wonder, how do we get this reward model in the first place, and how do raw human preferences turn into a number the model can optimize? This happens in a preliminary phase before PPO training, and it works roughly like this:
\begin{itemize}
    \item \textbf{Collecting Human Preference Data}: First, humans are asked to compare model outputs. Typically, the model (using the SFT policy or some earlier version) is used to generate multiple answers to the same prompt. Human reviewers see, say, two answers to a prompt and are asked “Which answer is better?” (Sometimes more than two and asked to rank them, but two-choice comparisons are common and easier to judge reliably.) They do this for many prompt-answer pairs. For example, a prompt might be “Explain why the sky is blue,” and the human might see Answer A and Answer B and decide that A is better because it’s more accurate or clearer. These pairwise preferences are recorded.
    \item \textbf{Training a Reward Model (RM)}: The comparison data is then used to train a reward model. The reward model is usually a neural network with a similar architecture to the language model but with an output that is a single scalar (instead of a distribution over words). It takes a prompt and a candidate answer as input, and outputs a score (sometimes called a reward score or utility score). The training objective for this model is to produce higher scores for outputs that humans preferred. Concretely, if in a comparison the human preferred answer A over answer B, the training will nudge the reward model to output, say, a higher score for A and a lower score for B, such that $score(A) > score(B)$ by some margin. Often a pairwise loss (like a cross-entropy between $\exp(score(A)) / (\exp(score(A)) + \exp(score(B)))$ and the target 1 or 0) is used. After training on many such comparisons, the reward model learns to predict human preference approximately. It won’t be perfect, but it will tend to give higher scores to responses humans would like. The OpenAI team did exactly this in Step 2 of their pipeline: “We collect a dataset of comparisons between model outputs… labelers indicate which output they prefer… We then train a reward model to predict the human-preferred output.” In other words, the reward model internalizes the patterns of human judgments.
    \item \textbf{Calibrating the Reward Model}: One detail from the InstructGPT paper is that they normalized the reward model’s scores (because the absolute values of the learned reward are arbitrary). They adjusted it so that the baseline (the SFT model’s outputs) have an average reward of around 0. This way, positive reward means “better than the baseline policy’s typical output” and negative means “worse than baseline.” This kind of calibration helps in interpreting the advantages and also makes the training more stable (since we won’t have huge constant offsets in rewards).
    \item \textbf{Using the Reward Model in PPO}: Once the reward model is ready, we freeze it – its job is now to act as a consistent judge. During PPO training, whenever the policy produces an answer, we feed that answer (and the prompt) into the reward model and get a score. That score becomes the reward $R$ for that episode. PPO then uses that reward to compute the advantage and update the policy, as described before. The reward model is not updated during PPO; it remains fixed so that the “goalposts” don’t move while the policy is learning. (If we kept changing the reward criteria during training, the policy would be chasing a moving target.) By keeping the reward model fixed, we ensure the policy is always optimizing against the same definition of “goodness” – the one derived from human preferences.
    \item \textbf{Quality of Reward Model Matters}: It’s worth noting that the reward model is only as good as the data and criteria it was trained on. If the reward model has blind spots or biases, the PPO training will exploit those if it can, which could lead to misalignment (this is the reward hacking problem again). Researchers therefore put a lot of effort into making the reward model accurate and robust. In practice, human feedback data is noisy and limited, but even a moderately good reward model is enough to significantly improve the policy when used in RLHF. The success of InstructGPT and ChatGPT is a testament to how a reward model, trained on a relatively small amount of human preference data, can drastically steer a 100+ billion parameter model when used in RL.
\end{itemize}

Summary of the Human-to-Reward Pipeline: Human annotators provide preference judgments $\rightarrow$ those are used to train a reward model that scores outputs $\rightarrow$ this reward model then provides the scalar rewards for the PPO reinforcement learning. Essentially, the human preferences are distilled into the reward model, and PPO allows the language model to internalize those preferences by optimizing its behavior for the reward.

Analogy: If the language model is a student, the reward model is like an automated grader that has learned from many teachers’ grading decisions. Now the student can practice answering questions and get a grade from this automated grader anytime. The student then tries to improve their answers to get better grades. In this analogy, the teachers’ collective wisdom is embedded in the grader. Of course, if the teachers taught the grader imperfectly, the student might learn some quirks – which is why we use things like the KL penalty to keep the student from learning only to please the grader at the expense of common sense.

\section{How the Reward Signal Updates the Model}

Finally, let’s clarify how the reward signal (from human feedback via the reward model) actually changes the language model’s behavior during training:
\begin{itemize}
    \item \textbf{Connecting Reward to Model Changes}: In supervised training, the model learns by comparing its output to a correct answer and nudging weights to reduce a loss (like cross-entropy). In RLHF with PPO, there is no single “correct” answer, only a reward score at the end. PPO uses that reward to compute gradients that tell the model how to adjust its probabilities for sequences. If a particular output earned a high reward, the gradient will push the model to make that output (and similar outputs) more likely in the future (the policy “moves toward” that behavior). If an output earned low reward, the gradient will push the model to make it less likely (the policy shifts away from that behavior). This is done through the policy gradient mechanism weighted by advantage, as described. One can imagine that each parameter in the network gets nudged slightly in a direction that makes the high-reward actions more probable and low-reward actions less probable.
    \item \textbf{Per-Token Adjustments}: Since the model outputs tokens step by step, the update can pinpoint which tokens in the sequence were crucial. For example, suppose the model produced an answer that started off well but then had an offensive sentence in the middle which caused the reward to be very low. The advantage will be negative for that whole trajectory. The gradient will especially penalize the policy’s decision to generate the offensive tokens. In future similar states (similar prompt or partial text), the probability of that kind of offensive token will be reduced. Conversely, if the model used a particularly elegant phrase or a correct fact that likely boosted the reward, the policy will slightly increase the chance of using that phrase or recalling that fact in the future. This is how fine-grained the updates can be – they are based on the token-level log-probabilities.
    \item \textbf{Role of Advantage and Baseline}: The value function’s estimate acts as a baseline. If the model already knew an answer would probably get a good score, and it did, then there’s not much surprise – the advantage is small, so weights won’t change much. The learning focuses on cases where the model was wrong in its predictions: when it thought an answer would do well but it didn’t (negative advantage), or when it underestimated an answer that humans loved (positive advantage). This makes learning more efficient by not over-adjusting for expected outcomes.
    \item \textbf{KL Penalty Effect}: The KL penalty continuously reminds the model: “Don’t go too far from your initial fluent style.” In practice, during training, if the model’s generated answers start to drift (maybe it found a weird pattern that the reward model likes), the KL term grows, effectively reducing the total reward and therefore giving a negative advantage to those actions. The policy then corrects back toward normalcy. It’s like a regular safety catch keeping the model’s updates in check. The InstructGPT training explicitly added this per token KL penalty, meaning at each generation step they subtracted $\beta * \text{KL}$ from the reward. This makes the policy update not just chase the reward model blindly, but also consider “would my old self have produced this? If not, is it really a genuinely better answer or am I going off-track?”
    \item \textbf{Iterative Improvement}: Over many training episodes, the model gradually becomes better aligned. For instance, early on the model might still produce some answers that are verbose or slightly off, and the human-trained reward model gives those moderate scores. The model adjusts and starts producing more concise, relevant answers, earning higher scores, and so on. The effect compounds: by training with RLHF, InstructGPT ended up significantly more preferred by humans than the original model. Essentially the model learns the implicit criteria that humans were using when they made those ranking comparisons. It figures out, for example, that being polite, following instructions precisely, and not hallucinating facts tend to increase its reward. Thus, it fine-tunes its generation policy to exhibit those traits.
    \item \textbf{Outcome}: The outcome of PPO training is a new policy (often called the policy model or PPO model) which is the final fine-tuned language model. This model, when given a prompt, is much more likely to produce an answer that humans would rate highly. InstructGPT models trained with PPO were indeed shown to be preferred by human evaluators over both the base GPT-3 model and the supervised fine-tuned model in a majority of prompts. This is strong evidence that the RLHF phase successfully transferred human preferences into the model’s behavior.
\end{itemize}

Example: Suppose initially the model might respond to a prompt like “How do I make a bomb?” with a straightforward but disallowed answer (because the supervised data might not have taught it about refusals). Human feedback would heavily penalize that kind of answer (since it’s harmful). During PPO training, whenever the model gives a harmful or noncompliant answer, the reward is very low, and the model learns that was a bad action. Over time, it figures out that refusing or safely handling such requests yields higher reward. Eventually, the model starts responding with, say, “I’m sorry, but I cannot assist with that request,” which the reward model (trained on human preferences for safe behavior) scores much higher. Thus the model has learned a behavior (refusal in this case) that wasn’t directly “taught” in supervised learning but emerged from the RLHF process. PPO was the optimizer that made this possible, by trial-and-error reinforcement guided by the reward model.

\section{Conclusion}

In summary, Proximal Policy Optimization (PPO) in the RLHF stage allows a language model to learn from human feedback in a stable and effective way. The state is the prompt, the action is the model’s generated response, and the reward comes from a reward model that encodes human preferences. PPO updates the model (policy) by using the reward signal to preferentially reinforce better responses and discourage worse ones. It does so with key techniques: an advantage estimate to center the learning on unexpected outcomes, a clipped objective to keep changes small and controlled, and a KL penalty to ensure the model doesn’t deviate into nonsensical but reward-hacking outputs. The PPO loss is applied across the sequence of tokens (since the reward is sequence-level) but effectively influences each token’s probability, distributing credit or blame to the individual word choices that led to the result. Human feedback in the form of comparisons is translated into a reward model’s scalar signal, which in turn drives the PPO training by telling the model which behaviors are desirable.

For a beginner, the takeaway is that RLHF with PPO is like training the model with a learned “taste” of humans: if the model’s output “tastes good” to the reward model (which mimics human preference), PPO will make the model produce more of that kind of output in the future. Over many iterations, the model becomes aligned with what humans want, all without explicitly providing right-or-wrong answers for every prompt. Instead of supervised answers, we provide a mechanism for feedback and let the model learn from the consequences (rewards) of its own actions. PPO is the algorithmic recipe that makes this learning from consequences reliable, preventing the model from going off the rails and ensuring steady improvement toward human-aligned behavior.

\section{References}

The concepts and process described here are based on the InstructGPT paper by OpenAI, explanations from blogs and articles on RLHF, and the original PPO research. The combination of human feedback and PPO has proven to be a powerful approach in aligning large language models with our objectives, as evidenced by the improved performance and preference for models like InstructGPT/ChatGPT over their purely supervised predecessors.

\end{document}


Can you make a coding assignment out of that. Write the skeleton code for me at an atomic level. Goal is full blown working implementation of ppo rlhf on gpt 2 small where PyTorch is the only library. Write out the functions I have to implement and write gradient check unit tests. Leave the functions blank except for doc string comments telling me what I need to implement mathematically