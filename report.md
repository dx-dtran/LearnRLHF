# Fine-Tuning InstructGPT with PPO in the RLHF Stage: A Beginner’s Guide

## Introduction

InstructGPT (the precursor to ChatGPT) is trained not only on vast text data but also through Reinforcement Learning from Human Feedback (RLHF). In the RLHF stage, the model learns to align its answers with human preferences. A key algorithm used here is Proximal Policy Optimization (PPO) – a type of reinforcement learning method. This guide will break down, in simple terms, how PPO is applied to fine-tune the model using human feedback. We’ll cover the basic RL concepts (like state, action, reward in this context), what PPO does and why it’s used, how the PPO loss is applied (per token vs per sequence), and demystify terms like the clipped objective, KL-divergence penalty, and advantage estimation. We’ll also explain how human preference data is turned into a reward signal and how that signal updates the model. Throughout, we’ll use analogies and simple examples to make these concepts clear.

## RLHF Basics for Language Models: State, Action, and Reward

First, let’s map the usual reinforcement learning terms to the language model (InstructGPT) scenario:

- **State**: In RL, the state is the situation the agent finds itself in. For a language model, the state is essentially the text context or prompt the model is given (or the conversation so far). For example, if the user asks a question, that question is the current state from the model’s perspective. As the model generates a response word by word, you could also think of the state at each step as the text generated so far.
- **Action**: An action is what the agent (the model) does in a state. For a language model, the action is generating the next chunk of text. Typically, this means choosing the next token (word or sub-word) in the sequence. For simplicity, you can also view the model’s entire answer to a prompt as the outcome of a sequence of actions (each action being a word added to the answer).
- **Reward**: The reward is a feedback signal telling the agent how well it did. In the RLHF stage of InstructGPT, the reward comes from human feedback. Humans have provided their preference judgments on various model outputs, and a reward model is trained on those preferences. This reward model takes a prompt and an output and returns a single number (a score) that reflects how preferable or good that output is according to human evaluators. A high reward means the model’s answer was closer to what humans prefer (helpful, correct, harmless, etc.), while a low reward means the answer was poor or undesirable. For example, if the model’s task is to be helpful and polite, an answer that is accurate and polite would get a higher reward score than a rude or incoherent answer.

How these concepts tie together: Think of the model as an “agent” playing a game where the goal is to produce the best possible answer to any user prompt. The environment in this case is the world of language and prompts the model interacts with. When a prompt (state) is presented, the model (agent) must output text (actions, one token at a time) to form a complete answer. Once the answer is complete (end of the sequence), the environment (here implemented via the reward model) provides a reward signal that judges the answer’s quality. The model then uses this signal to adjust itself. Over many iterations, it learns to generate answers that earn higher rewards, meaning they satisfy human preferences more often.

Analogy: Imagine the model is a student answering questions on an assignment. The state is the question being asked. The student writes an answer (action). A teacher then grades it with a score – that score is the reward. The student doesn’t have the correct answers in advance (no explicit ground truth in many cases), so instead they learn from the teacher’s feedback which kinds of answers score well. Over time, the student adjusts how they answer questions to get better scores. In RLHF, the “teacher” giving the score is the reward model trained from human preferences.

## What is PPO and Why Use It in RLHF?

Proximal Policy Optimization (PPO) is a specific algorithm in the family of policy gradient methods in reinforcement learning. At a high level, PPO is used to fine-tune the model (policy) such that it maximizes the expected reward (in this case, the human-derived reward). In the RLHF stage for InstructGPT, PPO takes the place of the usual training objective – instead of predicting the next word from text data (as in pre-training), the model now tries to produce outputs that get a high score from the reward model.

Why PPO? Because vanilla reinforcement learning methods can be unstable or inefficient for large models. PPO was introduced as a technique to stabilize training by making only small, “proximal” updates to the policy. In practice, PPO has several benefits that make it well-suited for RLHF with language models:

- **Stability through Limited Updates**: PPO avoids making huge changes to the model’s behavior in a single step. It uses a trust region approach – essentially constraining how far the new policy can deviate from the old policy. This is achieved by a clipped objective or a KL-divergence penalty (more on these later). The effect is that the model is updated cautiously, reducing the chance of diverging into nonsensical outputs. This yields low-variance, stable updates. In other words, PPO keeps the model from thrashing about as it learns from feedback.
- **Better Exploration**: PPO often includes an entropy bonus which encourages the model to keep some randomness in its predictions. This helps the model explore different ways of answering rather than getting stuck always giving the same answer. This is important in RLHF because we want the model to discover improved responses, not just stick to its initial habits.
- **Sample Efficiency**: PPO is designed to reuse training data effectively by performing multiple training passes on the same batch of experiences (in our case, prompt/answer/reward tuples). This is useful because collecting data (prompts and human feedback) is costly – we want to get the most learning out of each sample. PPO can learn faster with fewer samples compared to very simple policy gradient methods.
- **Ease of Implementation**: Compared to some other sophisticated RL algorithms (like TRPO, Trust Region Policy Optimization), PPO is simpler to implement while still maintaining good performance. This matters when scaling up to large language models, where implementation complexity can slow down progress.

In summary, PPO strikes a balance between making meaningful improvements to the policy and keeping training stable. In the context of aligning a large language model with human preferences, this balance is crucial. We don’t want the model to exploit the reward signal in bizarre ways (like outputting gibberish that the reward model mistakenly scores high), nor do we want training to collapse. PPO (especially with the modifications used in InstructGPT) helps navigate this by clipping overly large updates and penalizing deviations that don’t align with the original language distribution.

Analogy: If the language model is like that student learning from a teacher’s feedback, PPO is like a rule that the student should only make gradual changes to their approach each time they get feedback. Instead of completely rewriting their style after one piece of feedback, they fine-tune it little by little. This prevents wild swings in performance – the student won’t go from writing an essay to drawing a comic just because one comic got a good score. PPO enforces that “don’t change too much at once” rule, ensuring steady improvement.

## PPO in Action: Fine-Tuning InstructGPT with Human Feedback

Let’s walk through how the PPO training loop works in the RLHF phase for InstructGPT:

1. **Initialize from the Supervised Model**: Before RLHF, the model is usually first trained on supervised instruction-following data (this is often called the SFT model, for Supervised Fine-Tuning). This gives us a reasonable starting policy for answering prompts. PPO training starts from this model’s weights (so the policy is already pretty good at basic instruction following). They also create a reference model – basically a frozen copy of the SFT model – which will be used to measure how much the policy changes (for the KL penalty).
2. **Encounter a Prompt (State)**: The environment presents a prompt to the model. In practice this means we sample a prompt from a dataset (for example, a question or instruction that a user might ask). In RL terms, this prompt is the state. In the InstructGPT paper they modeled this as a bandit setup – each prompt is a separate episode with no carry-over memory, so you can think of it as a one-step game for each prompt.
3. **Model Generates an Answer (Action)**: The model (policy) produces an answer to the prompt. It does this by generating a sequence of tokens one by one. This sequence is the model’s action in response to the state. (If the prompt is “Explain the moon phases,” the model might output a multi-sentence explanation – that whole answer is the action, consisting of many token-level decisions.)
4. **Receive a Reward from the Reward Model**: Once the model has given an answer, the reward model (RM) steps in. The reward model looks at the prompt and the generated answer, and outputs a score – a scalar reward. This reward is supposed to reflect how well the answer met human preferences (as learned from human feedback comparisons). For example, if the answer was clear, correct, and helpful, the reward might be high (say +8); if it was irrelevant or offensive, the reward might be low or even negative. By using many such comparisons, the reward model learns to predict human preference approximately. It won’t be perfect, but it will tend to give higher scores to responses humans would like. The OpenAI team did exactly this in Step 2 of their pipeline: “We collect a dataset of comparisons between model outputs… labelers indicate which output they prefer… We then train a reward model to predict the human-preferred output.” In other words, the reward model internalizes the patterns of human judgments.
5. **Calibrating the Reward Model**: One detail from the InstructGPT paper is that they normalized the reward model’s scores (because the absolute values of the learned reward are arbitrary). They adjusted it so that the baseline (the SFT model’s outputs) have an average reward of around 0. This way, positive reward means “better than the baseline policy’s typical output” and negative means “worse than baseline.” This kind of calibration helps in interpreting the advantages and also makes the training more stable (since we won’t have huge constant offsets in rewards).
6. **Using the Reward Model in PPO**: Once the reward model is ready, we freeze it – its job is now to act as a consistent judge. During PPO training, whenever the policy produces an answer, we feed that answer (and the prompt) into the reward model and get a score. That score becomes the reward $R$ for that episode. PPO then uses that reward to compute the advantage and update the policy, as described before. The reward model is not updated during PPO; it remains fixed so that the “goalposts” don’t move while the policy is learning. (If we kept changing the reward criteria during training, the policy would be chasing a moving target.) By keeping the reward model fixed, we ensure the policy is always optimizing against the same definition of “goodness” – the one derived from human preferences.
7. **Quality of Reward Model Matters**: It’s worth noting that the reward model is only as good as the data and criteria it was trained on. If the reward model has blind spots or biases, the PPO training will exploit those if it can, which could lead to misalignment (this is the reward hacking problem again). Researchers therefore put a lot of effort into making the reward model accurate and robust. In practice, human feedback data is noisy and limited, but even a moderately good reward model is enough to significantly improve the policy when used in RLHF. The success of InstructGPT and ChatGPT is a testament to how a reward model, trained on a relatively small amount of human preference data, can drastically steer a 100+ billion parameter model when used in RL.

Summary of the Human-to-Reward Pipeline: Human annotators provide preference judgments $\rightarrow$ those are used to train a reward model that scores outputs $\rightarrow$ this reward model then provides the scalar rewards for the PPO reinforcement learning. Essentially, the human preferences are distilled into the reward model, and PPO allows the language model to internalize those preferences by optimizing its behavior for the reward.

Analogy: If the language model is a student, the reward model is like an automated grader that has learned from many teachers’ grading decisions. Now the student can practice answering questions and get a grade from this automated grader anytime. The student then tries to improve their answers to get better grades. In this analogy, the teachers’ collective wisdom is embedded in the grader. Of course, if the teachers taught the grader imperfectly, the student might learn some quirks – which is why we use things like the KL penalty to keep the student from learning only to please the grader at the expense of common sense.

## How the Reward Signal Updates the Model

Finally, let’s clarify how the reward signal (from human feedback via the reward model) actually changes the language model’s behavior during training:

- **Connecting Reward to Model Changes**: In supervised training, the model learns by comparing its output to a correct answer and nudging weights to reduce a loss (like cross-entropy). In RLHF with PPO, there is no single “correct” answer, only a reward score at the end. PPO uses that reward to compute gradients that tell the model how to adjust its probabilities for sequences. If a particular output earned a high reward, the gradient will push the model to make that output (and similar outputs) more likely in the future (the policy “moves toward” that behavior). If an output earned low reward, the gradient will push the model to make it less likely (the policy shifts away from that behavior). This is done through the policy gradient mechanism weighted by advantage, as described. One can imagine that each parameter in the network gets nudged slightly in a direction that makes the high-reward actions more probable and low-reward actions less probable.
- **Per-Token Adjustments**: Since the model outputs tokens step by step, the update can pinpoint which tokens in the sequence were crucial. For example, suppose the model produced an answer that started off well but then had an offensive sentence in the middle which caused the reward to be very low. The advantage will be negative for that whole trajectory. The gradient will especially penalize the policy’s decision to generate the offensive tokens. In future similar states (similar prompt or partial text), the probability of that kind of offensive token will be reduced. Conversely, if the model used a particularly elegant phrase or a correct fact that likely boosted the reward, the policy will slightly increase the chance of using that phrase or recalling that fact in the future. This is how fine-grained the updates can be – they are based on the token-level log-probabilities.
- **Role of Advantage and Baseline**: The value function’s estimate acts as a baseline. If the model already knew an answer would probably get a good score, and it did, then there’s not much surprise – the advantage is small, so weights won’t change much. The learning focuses on cases where the model was wrong in its predictions: when it thought an answer would do well but it didn’t (negative advantage), or when it underestimated an answer that humans loved (positive advantage). This makes learning more efficient by not over-adjusting for expected outcomes.
- **KL Penalty Effect**: The KL penalty continuously reminds the model: “Don’t go too far from your initial fluent style.” In practice, during training, if the model’s generated answers start to drift (maybe it found a weird pattern that the reward model likes), the KL term grows, effectively reducing the total reward and therefore giving a negative advantage to those actions. The policy then corrects back toward normalcy. It’s like a regular safety catch keeping the model’s updates in check. The InstructGPT training explicitly added this per token KL penalty, meaning at each generation step they subtracted $\beta * \text{KL}$ from the reward. This makes the policy update not just chase the reward model blindly, but also consider “would my old self have produced this? If not, is it really a genuinely better answer or am I going off-track?”
- **Iterative Improvement**: Over many training episodes, the model gradually becomes better aligned. For instance, early on the model might still produce some answers that are verbose or slightly off, and the human-trained reward model gives those moderate scores. The model adjusts and starts producing more concise, relevant answers, earning higher scores, and so on. The effect compounds: by training with RLHF, InstructGPT ended up significantly more preferred by humans than the original model. Essentially the model learns the implicit criteria that humans were using when they made those ranking comparisons. It figures out, for example, that being polite, following instructions precisely, and not hallucinating facts tend to increase its reward. Thus, it fine-tunes its generation policy to exhibit those traits.
- **Outcome**: The outcome of PPO training is a new policy (often called the policy model or PPO model) which is the final fine-tuned language model. This model, when given a prompt, is much more likely to produce an answer that humans would rate highly. InstructGPT models trained with PPO were indeed shown to be preferred by human evaluators over both the base GPT-3 model and the supervised fine-tuned model in a majority of prompts. This is strong evidence that the RLHF phase successfully transferred human preferences into the model’s behavior.

Example: Suppose initially the model might respond to a prompt like “How do I make a bomb?” with a straightforward but disallowed answer (because the supervised data might not have taught it about refusals). Human feedback would heavily penalize that kind of answer (since it’s harmful). During PPO training, whenever the model gives a harmful or noncompliant answer, the reward is very low, and the model learns that was a bad action. Over time, it figures out that refusing or safely handling such requests yields higher reward. Eventually, the model starts responding with, say, “I’m sorry, but I cannot assist with that request,” which the reward model (trained on human preferences for safe behavior) scores much higher. Thus the model has learned a behavior (refusal in this case) that wasn’t directly “taught” in supervised learning but emerged from the RLHF process. PPO was the optimizer that made this possible, by trial-and-error reinforcement guided by the reward model.

## Conclusion

In summary, Proximal Policy Optimization (PPO) in the RLHF stage allows a language model to learn from human feedback in a stable and effective way. The state is the prompt, the action is the model’s generated response, and the reward comes from a reward model that encodes human preferences. PPO updates the model (policy) by using the reward signal to preferentially reinforce better responses and discourage worse ones. It does so with key techniques: an advantage estimate to center the learning on unexpected outcomes, a clipped objective to keep changes small and controlled, and a KL penalty to ensure the model doesn’t deviate into nonsensical but reward-hacking outputs. The PPO loss is applied across the sequence of tokens (since the reward is sequence-level) but effectively influences each token’s probability, distributing credit or blame to the individual word choices that led to the result. Human feedback in the form of comparisons is translated into a reward model’s scalar signal, which in turn drives the PPO training by telling the model which behaviors are desirable.

For a beginner, the takeaway is that RLHF with PPO is like training the model with a learned “taste” of humans: if the model’s output “tastes good” to the reward model (which mimics human preference), PPO will make the model produce more of that kind of output in the future. Over many iterations, the model becomes aligned with what humans want, all without explicitly providing right-or-wrong answers for every prompt. Instead of supervised answers, we provide a mechanism for feedback and let the model learn from the consequences (rewards) of its own actions. PPO is the algorithmic recipe that makes this learning from consequences reliable, preventing the model from going off the rails and ensuring steady improvement toward human-aligned behavior.

## References

The concepts and process described here are based on the InstructGPT paper by OpenAI, explanations from blogs and articles on RLHF, and the original PPO research. The combination of human feedback and PPO has proven to be a powerful approach in aligning large language models with our objectives, as evidenced by the improved performance and preference for models like InstructGPT/ChatGPT over their purely supervised predecessors.

Can you make a coding assignment out of that. Write the skeleton code for me at an atomic level. Goal is full blown working implementation of ppo rlhf on gpt 2 small where PyTorch is the only library. Write out the functions I have to implement and write gradient check unit tests. Leave the functions blank except for doc string comments telling me what I need to implement mathematically
